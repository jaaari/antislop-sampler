{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies:\n",
    "\n",
    "#!pip install transformers ipywidgets IPython\n",
    "\n",
    "# If running on Colab:\n",
    "#!git clone https://github.com/sam-paech/antislop-sampler.git\n",
    "#!mv antislop-sampler/src .\n",
    "#!mv antislop-sampler/slop_phrase_prob_adjustments.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "from src.antislop_generate import generate_antislop, chat_antislop\n",
    "\n",
    "# Enable efficient transfer for Hugging Face models\n",
    "os.environ['HF_HUB_ENABLE_HF_TRANSFER'] = \"1\"\n",
    "\n",
    "# Set the device to 'cuda' if available, else 'cpu'\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Specify the model name (replace with your preferred model)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct\"\n",
    "#model_name = \"unsloth/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Load the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16, trust_remote_code=True)\n",
    "model.to(device)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if os.path.exists('banned_tokens.json'):\n",
    "    with open('banned_tokens.json', 'r') as f:\n",
    "        slop_phrase_prob_adjustments = dict(json.load(f)[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write a story about Elara, the weaver of tapestries in future Technopolis. In the bustling city, a group of \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In the heart of the vast, sprawling city of Zha'thik, where futuristic skyscrapers pierced the sky and neon lights danced across the streets, there lived a young woman named Eldrida. She was known throughout the city as the 'Bridger of the Past,' a skilled artisan renowned for her extraordinary talent in transforming ancient wisdom into breathtaking works of art.\n",
      "\n",
      "Eldrida's workshop, aptly named 'The Lost Threads,' was a cozy, dimly lit sanctuary in the midst of the chaotic city. The air was thick with the scent of old books, wood shavings, and the faint hint of magic. The walls were adorned with stunning, hand-embroidered samples of her previous work, each one telling a story of a bygone era.\n",
      "\n",
      "One ordinary day, as Eldrida was busy gluing threads onto a new canvas, a group of five individuals entered her workshop. They were a diverse bunch, each with their own unique skills and stories to share. There was Arin, a young inventor with a passion for robotics; Zephyr, a charismatic performer with a flair for the dramatic; Lux, a brilliant scientist with a talent for cryptography; Mira, a skilled warrior with unparalleled combat prowess; and Jax, a charismatic smuggler with a network of underworld connections.\n",
      "\n",
      "The group had gathered at Eldrida's workshop, seeking her expertise in creating a massive, city-wide mural that would celebrate the anniversary of Zha'thik's founding. The city's leaders had commissioned the project, and Eldrida was the only one who could bring it to life.\n",
      "\n",
      "As the group began to discuss their ideas, Eldrida listened with a discerning ear, taking note of their diverse perspectives. Arin proposed a futuristic cityscape, while Zephyr suggested a whimsical, fantastical world. Lux advocated for a scientific, data-driven approach, while Mira envisioned a battle-hardened, warrior-inspired design. Jax, ever the pragmatist, suggested a gritty, realistic representation of the city's underworld.\n",
      "\n",
      "Eldrida listened patiently, her mind racing with the possibilities. She knew that the mural would be a masterpiece, but she also knew that it would require more than just artistic talent. It would demand collaboration, creativity, and a deep sense of empathy.\n",
      "\n",
      "As the group continued to brainstorm, Eldrida began to piece together a vision. She envisioned a mural that would capture the essence of Zha'thik's history, its triumphs, and its struggles. It would be a city-wide celebration, a nod to the past while looking toward the future.\n",
      "\n",
      "With her plan in place, Eldrida invited the group to join her in creating the mural. Together, they set to work, their diverse skills and talents blending together in a beautiful, chaotic dance. As the days passed, the mural began to take shape, a breathtaking work of art that told the story of Zha'thik"
     ]
    }
   ],
   "source": [
    "# Chat generation with streaming\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "tokens = []\n",
    "text = ''\n",
    "for token in chat_antislop(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    messages=messages,\n",
    "    max_new_tokens=600,\n",
    "    # Antislop sampling may be less reliable at low temperatures.\n",
    "    #temperature=1,    \n",
    "    #min_p=0.1,\n",
    "    temperature=0.01,\n",
    "    # The adjustment_strength param scales how strongly the probability adjustments are applied.\n",
    "    # A value of 1 means the values in slop_phrase_prob_adjustments (or the defaults) are used unmodified.\n",
    "    # Reasonable values are 0 (disabled) thru 100+ (effectively banning the list).\n",
    "    adjustment_strength=100.0,\n",
    "    # Optional: Provide a list of slop phrases and probability adjustments\n",
    "    slop_phrase_prob_adjustments=slop_phrase_prob_adjustments,\n",
    "    enforce_json=False,\n",
    "    antislop_enabled=True,\n",
    "    ban_slop_first_tokens=False,\n",
    "    streaming=True,\n",
    "    stream_smoothing=True, # On by default; this will smooth out the stutters from backtracking.\n",
    "):\n",
    "    tokens.append(token)\n",
    "    full_text = tokenizer.decode(tokens, skip_special_tokens=True)\n",
    "    new_text = full_text[len(text):]\n",
    "    text = full_text\n",
    "    print(new_text, end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
